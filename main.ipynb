{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Student Graduation Prediction\n",
    "\n",
    "In this notebook we will look at creating a good model based on the baseline model in the baseline notebook."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:28:50.821552Z",
     "start_time": "2025-10-17T17:28:50.261349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:28:52.828677Z",
     "start_time": "2025-10-17T17:28:52.811461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/data.csv',sep=\";\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Target'])\n",
    "class_names = list(le.classes_)\n",
    "X = df.drop(columns=['Target'])\n"
   ],
   "id": "1a2aded3bcfafbc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering\n",
    "Feature engineering is especially important in this dataset as there are many features which in themselves doesn't mean much, but when combined provides more value."
   ],
   "id": "fb47d3780fdc0033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:28:54.436464Z",
     "start_time": "2025-10-17T17:28:54.423871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # ratios\n",
    "        X[\"approval_ratio_1st\"] = X[\"Curricular units 1st sem (approved)\"] / X[\"Curricular units 1st sem (enrolled)\"].replace(0, np.nan)\n",
    "        X[\"approval_ratio_2nd\"] = X[\"Curricular units 2nd sem (approved)\"] / X[\"Curricular units 2nd sem (enrolled)\"].replace(0, np.nan)\n",
    "        # avg grade across semesters (ignore zeros)\n",
    "        X[\"avg_grade\"] = X[[\"Curricular units 1st sem (grade)\", \"Curricular units 2nd sem (grade)\"]].replace(0, np.nan).mean(axis=1)\n",
    "        # totals & overall ratio\n",
    "        X[\"total_approved\"] = X[\"Curricular units 1st sem (approved)\"] + X[\"Curricular units 2nd sem (approved)\"]\n",
    "        X[\"total_enrolled\"] = X[\"Curricular units 1st sem (enrolled)\"] + X[\"Curricular units 2nd sem (enrolled)\"]\n",
    "        X[\"total_approval_ratio\"] = X[\"total_approved\"] / X[\"total_enrolled\"].replace(0, np.nan)\n",
    "        return X.fillna(0.0)\n",
    "\n",
    "X = X.fillna(0)"
   ],
   "id": "723fc26017957a0a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "The dataset has already been cleaned by the dataset authors, so no preprocessing in the form of checking missing values etc. is needed.\n",
    "\n",
    "Though preprocessing in the form of scaling and encoding categorical values is not needed when it comes to gradient boosting models, and especially not in this dataset when it's already been done by the authors. It could be informative to test whether doing this preprocessing could help give better results."
   ],
   "id": "73cd8fa24dffc659"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:02.678974Z",
     "start_time": "2025-10-17T17:29:02.670242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "categorical_cols = [\n",
    "    \"Application mode\", \"Application order\", \"Course\",\n",
    "    \"Previous qualification\", \"Nationality\", \"Mother's qualification\",\n",
    "    \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\",\n",
    "    \"Displaced\", \"Debtor\", \"Tuition fees up to date\", \"Scholarship holder\",\n",
    "    \"Gender\", \"International\", \"Marital status\", \"Daytime/evening attendance\\t\",\n",
    "    \"Educational special needs\"\n",
    "]\n",
    "categorical_cols = [c for c in categorical_cols if c in X.columns]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")"
   ],
   "id": "f62f2bb8c7d54a43",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training a Model\n",
    "We will be using different gradient boosting models like XGBoost, LightGBM and CatBoost for this. In addition, some of the pipelines beneath uses SMOTE, and some don't, this is simply because some did better (in the form of accuracy) with, and some without."
   ],
   "id": "94fe6b4a4f13e71d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:04.059192Z",
     "start_time": "2025-10-17T17:29:04.045609Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)",
   "id": "55515e2ce5a3768d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:05.909550Z",
     "start_time": "2025-10-17T17:29:04.904197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = np.unique(y_train)\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = (class_counts.sum() / (len(classes) * class_counts))\n",
    "sample_weight = class_weights[y_train]\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.08,\n",
    "    max_depth=6,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(classes),\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "pipe_xgb = ImbPipeline([\n",
    "    (\"fe\", FeatureEngineer()),\n",
    "    # (\"pre\", preprocess),\n",
    "    (\"clf\", xgb),\n",
    "])\n",
    "\n",
    "pipe_xgb.fit(X_train, y_train)\n",
    "y_pred = pipe_xgb.predict(X_test)\n",
    "\n",
    "print(\"=== XGBoost ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ],
   "id": "75cad7bcf795c742",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost ===\n",
      "Accuracy: 0.7774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.82      0.77      0.79       284\n",
      "    Enrolled       0.54      0.47      0.51       159\n",
      "    Graduate       0.82      0.89      0.86       442\n",
      "\n",
      "    accuracy                           0.78       885\n",
      "   macro avg       0.73      0.71      0.72       885\n",
      "weighted avg       0.77      0.78      0.77       885\n",
      "\n",
      "Confusion matrix:\n",
      " [[218  29  37]\n",
      " [ 35  75  49]\n",
      " [ 13  34 395]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:12.391338Z",
     "start_time": "2025-10-17T17:29:08.549354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe_xgb_smote = ImbPipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),\n",
    "    # (\"pre\", preprocess),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"clf\", xgb),\n",
    "])\n",
    "\n",
    "pipe_xgb_smote.fit(X_train, y_train)\n",
    "y_pred_sm = pipe_xgb_smote.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost + SMOTE ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred_sm):.4f}\")\n",
    "print(classification_report(y_test, y_pred_sm, target_names=class_names))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_sm))"
   ],
   "id": "26217fc749cd06e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\tdt4259\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost + SMOTE ===\n",
      "Accuracy: 0.7627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.80      0.73      0.76       284\n",
      "    Enrolled       0.51      0.50      0.50       159\n",
      "    Graduate       0.83      0.88      0.85       442\n",
      "\n",
      "    accuracy                           0.76       885\n",
      "   macro avg       0.71      0.70      0.71       885\n",
      "weighted avg       0.76      0.76      0.76       885\n",
      "\n",
      "Confusion matrix:\n",
      " [[208  40  36]\n",
      " [ 35  80  44]\n",
      " [ 17  38 387]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:17.611804Z",
     "start_time": "2025-10-17T17:29:15.983508Z"
    }
   },
   "source": [
    "catboost_clf = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function=\"MultiClass\",\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "pipe_catboost_smote = ImbPipeline([\n",
    "    (\"fe\", FeatureEngineer()),\n",
    "    (\"clf\", catboost_clf),\n",
    "])\n",
    "\n",
    "pipe_catboost_smote.fit(X_train, y_train)\n",
    "y_pred_cat = pipe_catboost_smote.predict(X_test)\n",
    "\n",
    "print(\"\\n=== CatBoost ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred_cat):.4f}\")\n",
    "print(classification_report(y_test, y_pred_cat, target_names=class_names))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_cat))"
   ],
   "id": "84c71e8d5bf6a4e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n",
      "Accuracy: 0.7819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.80      0.77      0.79       284\n",
      "    Enrolled       0.57      0.43      0.49       159\n",
      "    Graduate       0.82      0.92      0.87       442\n",
      "\n",
      "    accuracy                           0.78       885\n",
      "   macro avg       0.73      0.71      0.72       885\n",
      "weighted avg       0.77      0.78      0.77       885\n",
      "\n",
      "Confusion matrix:\n",
      " [[218  26  40]\n",
      " [ 41  69  49]\n",
      " [ 12  25 405]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:29:22.246618Z",
     "start_time": "2025-10-17T17:29:20.770741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lgbm_clf = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "pipe_lgbm = ImbPipeline([\n",
    "    (\"fe\", FeatureEngineer()),\n",
    "    (\"clf\", lgbm_clf),\n",
    "])\n",
    "\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "y_pred_lgbm = pipe_lgbm.predict(X_test)\n",
    "\n",
    "print(\"\\n=== LightGBM ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(classification_report(y_test, y_pred_lgbm, target_names=class_names))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_lgbm))"
   ],
   "id": "3b2631f3bcc0baec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LightGBM ===\n",
      "Accuracy: 0.7763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.82      0.76      0.79       284\n",
      "    Enrolled       0.54      0.47      0.51       159\n",
      "    Graduate       0.82      0.89      0.85       442\n",
      "\n",
      "    accuracy                           0.78       885\n",
      "   macro avg       0.73      0.71      0.72       885\n",
      "weighted avg       0.77      0.78      0.77       885\n",
      "\n",
      "Confusion matrix:\n",
      " [[217  30  37]\n",
      " [ 33  75  51]\n",
      " [ 14  33 395]]\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
